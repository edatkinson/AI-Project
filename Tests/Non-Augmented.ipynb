{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "#pip install opencv-python (for cv2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "######Stratified Sampling:########\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='/users/edatkinson/LLL/split_classes/train/', transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root='/users/edatkinson/LLL/split_classes/validation/', transform=transform)\n",
    "\n",
    "# Calculate weights for each class\n",
    "train_targets = torch.tensor(train_dataset.targets)\n",
    "class_sample_count = torch.tensor([(train_targets == t).sum() for t in torch.unique(train_targets, sorted=True)])\n",
    "print(class_sample_count)\n",
    "weight = 1. / class_sample_count.float()\n",
    "print(weight)\n",
    "samples_weight = torch.tensor([weight[t] for t in train_targets])\n",
    "# WeightedRandomSampler for training set\n",
    "train_sampler = WeightedRandomSampler(weights=samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "\n",
    "# For validation, you can repeat the process if you want stratified sampling there as well\n",
    "# val_targets = torch.tensor(val_dataset.targets)\n",
    "# val_class_sample_count = torch.tensor([(val_targets == t).sum() for t in torch.unique(val_targets, sorted=True)])\n",
    "# val_weight = 1. / val_class_sample_count.float()\n",
    "# val_samples_weight = torch.tensor([val_weight[t] for t in val_targets])\n",
    "\n",
    "# val_sampler = WeightedRandomSampler(weights=val_samples_weight, num_samples=len(val_samples_weight), replacement=True)\n",
    "\n",
    "# Data loaders with stratified sampling\n",
    "train_loader = DataLoader(train_dataset, batch_size=32,sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 182,  208,  464, 9661,  398])\n",
      "tensor([0.0055, 0.0048, 0.0022, 0.0001, 0.0025])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Adding pooling layer\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Adding pooling layer\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Adding pooling layer\n",
    "        \n",
    "        # After 3 rounds of halving the dimensions, the size calculation needs adjustment\n",
    "        # For an input of 224x224, after three poolings, the size is 224 / 2 / 2 / 2 = 28\n",
    "        self.fc1_size = 64 * 28 * 28  # Adjusted based on the pooling layers\n",
    "        self.fc1 = nn.Linear(self.fc1_size, 500)\n",
    "        self.fc2 = nn.Linear(500, 5)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))  # Apply pooling\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))  # Apply pooling\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))  # Apply pooling\n",
    "        x = x.view(-1, self.fc1_size)  # Flatten the output for the fully connected layer\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#custom cross entropy loss function\n",
    "\n",
    "class WeightedCrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # The F.cross_entropy function already applies log_softmax internally,\n",
    "        # so we pass the raw scores directly.\n",
    "        return F.cross_entropy(input, target, weight=self.class_weights)\n",
    "\n",
    "\n",
    "class_counts = class_sample_count  # counts for classes\n",
    "print(class_counts)\n",
    "total_count = sum(class_counts)\n",
    "classes_weights = torch.tensor([total_count / c for c in class_counts], dtype=torch.float32)\n",
    "classes_weights /= classes_weights.min() #normalize the weights\n",
    "print(f'Normalised Class Weights, Need to play around with these:{classes_weights}')\n",
    "# If you're using CUDA, move the weights to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    classes_weights = classes_weights.cuda()\n",
    "\n",
    "\n",
    "# Create the model, loss function, and optimizer\n",
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = WeightedCrossEntropyLoss(classes_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 182,  208,  464, 9661,  398])\n",
      "Normalised Class Weights, Need to play around with these:tensor([53.0824, 46.4471, 20.8211,  1.0000, 24.2739])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Load test data\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='/users/edatkinson/LLL/split_classes/test/', transform=transform)\n",
    "#print(test_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# if not train_on_gpu:\n",
    "#     print('CUDA is not available.  Training on CPU ...')\n",
    "# else:\n",
    "#     print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "# if train_on_gpu:\n",
    "#     model.cuda()\n",
    "\n",
    "# n_epochs = 2\n",
    "# print_every = 50\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     train_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "#         if train_on_gpu:\n",
    "#             images, labels = images.cuda(), labels.cuda()  # Move data to GPU\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item() * images.size(0)\n",
    "#         _, predicted = torch.max(output, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         if (batch_idx + 1) % print_every == 0:\n",
    "#             accuracy = 100 * correct / total\n",
    "#             print(f\"Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Training Loss: {train_loss / total}, Training Accuracy: {accuracy}%\")\n",
    "    \n",
    "#     train_loss = train_loss / len(train_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss}, Training Accuracy: {accuracy}%\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming `model`, `train_loader`, `test_loader`, `optimizer`, `criterion` are defined\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "n_epochs = 2\n",
    "print_every = 50\n",
    "metrics = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        if train_on_gpu:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate metrics after each batch\n",
    "        batch_accuracy = 100 * correct / total\n",
    "        batch_loss = train_loss / total\n",
    "        \n",
    "        # Optionally, evaluate test accuracy at a reduced frequency to save computation\n",
    "        test_accuracy = None\n",
    "        if (batch_idx + 1) % print_every == 0:  # Example: Calculate test accuracy every 'print_every' batches\n",
    "            model.eval()  # Set model to evaluate mode\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            with torch.no_grad():\n",
    "                for test_images, test_labels in test_loader:\n",
    "                    if train_on_gpu:\n",
    "                        test_images, test_labels = test_images.cuda(), test_labels.cuda()\n",
    "                    test_outputs = model(test_images)\n",
    "                    _, test_predicted = torch.max(test_outputs, 1)\n",
    "                    test_total += test_labels.size(0)\n",
    "                    test_correct += (test_predicted == test_labels).sum().item()\n",
    "            test_accuracy = 100 * test_correct / test_total\n",
    "            model.train()  # Set model back to train mode\n",
    "\n",
    "        metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'batch': batch_idx + 1,\n",
    "            'train_loss': batch_loss,\n",
    "            'train_accuracy': batch_accuracy,\n",
    "            'test_accuracy': test_accuracy  # This will be None if not computed\n",
    "        })\n",
    "        \n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Training Loss: {batch_loss}, Training Accuracy: {batch_accuracy}%, Test Accuracy: {'N/A' if test_accuracy is None else f'{test_accuracy}%'}\")\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    epoch_loss = train_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, End of Epoch, Training Loss: {epoch_loss}, Training Accuracy: {epoch_accuracy}%\")\n",
    "\n",
    "# Convert metrics to DataFrame and save as CSV\n",
    "df = pd.DataFrame(metrics)\n",
    "df.to_csv('training_testing_metrics.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA is not available.  Training on CPU ...\n",
      "Epoch 1/2, Batch 50/342, Training Loss: 0.9773583817481994, Training Accuracy: 57.75%, Test Accuracy: 83.90052356020942%\n",
      "Epoch 1/2, Batch 100/342, Training Loss: 0.9878801548480988, Training Accuracy: 58.90625%, Test Accuracy: 84.29319371727749%\n",
      "Epoch 1/2, Batch 150/342, Training Loss: 0.9640447012583415, Training Accuracy: 59.520833333333336%, Test Accuracy: 81.93717277486911%\n",
      "Epoch 1/2, Batch 200/342, Training Loss: 0.9378046801686287, Training Accuracy: 60.640625%, Test Accuracy: 84.03141361256544%\n",
      "Epoch 1/2, Batch 250/342, Training Loss: 0.9016932303905487, Training Accuracy: 62.1625%, Test Accuracy: 85.34031413612566%\n",
      "Epoch 1/2, Batch 300/342, Training Loss: 0.8646099028984705, Training Accuracy: 64.125%, Test Accuracy: 84.42408376963351%\n",
      "Epoch 1/2, End of Epoch, Training Loss: 0.8419335980818644, Training Accuracy: 65.08751030880602%\n",
      "Epoch 2/2, Batch 50/342, Training Loss: 0.7366245907545089, Training Accuracy: 69.5%, Test Accuracy: 83.37696335078535%\n",
      "Epoch 2/2, Batch 100/342, Training Loss: 0.6651171141862869, Training Accuracy: 72.5%, Test Accuracy: 84.42408376963351%\n",
      "Epoch 2/2, Batch 150/342, Training Loss: 0.6301859766244888, Training Accuracy: 74.08333333333333%, Test Accuracy: 84.29319371727749%\n",
      "Epoch 2/2, Batch 200/342, Training Loss: 0.5985562391579151, Training Accuracy: 75.4375%, Test Accuracy: 77.61780104712042%\n",
      "Epoch 2/2, Batch 250/342, Training Loss: 0.5726015480160713, Training Accuracy: 76.5125%, Test Accuracy: 84.16230366492147%\n",
      "Epoch 2/2, Batch 300/342, Training Loss: 0.5540710502366225, Training Accuracy: 77.45833333333333%, Test Accuracy: 85.07853403141361%\n",
      "Epoch 2/2, End of Epoch, Training Loss: 0.5454502838591132, Training Accuracy: 77.92541006139467%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# if not train_on_gpu:\n",
    "#     print('CUDA is not available.  Training on CPU ...')\n",
    "# else:\n",
    "#     print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "# if train_on_gpu:\n",
    "#     model.cuda()\n",
    "\n",
    "# n_epochs = 2\n",
    "# print_every = 50\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     train_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "#         if train_on_gpu:\n",
    "#             images, labels = images.cuda(), labels.cuda()  # Move data to GPU\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item() * images.size(0)\n",
    "#         _, predicted = torch.max(output, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         if (batch_idx + 1) % print_every == 0:\n",
    "#             accuracy = 100 * correct / total\n",
    "#             print(f\"Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Training Loss: {train_loss / total}, Training Accuracy: {accuracy}%\")\n",
    "    \n",
    "#     train_loss = train_loss / len(train_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss}, Training Accuracy: {accuracy}%\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming `model`, `train_loader`, `test_loader`, `optimizer`, `criterion` are defined\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "n_epochs = 2\n",
    "print_every = 50\n",
    "metrics = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        if train_on_gpu:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate metrics after each batch\n",
    "        batch_accuracy = 100 * correct / total\n",
    "        batch_loss = train_loss / total\n",
    "        \n",
    "        # Optionally, evaluate test accuracy at a reduced frequency to save computation\n",
    "        test_accuracy = None\n",
    "        if (batch_idx + 1) % print_every == 0:  # Example: Calculate test accuracy every 'print_every' batches\n",
    "            model.eval()  # Set model to evaluate mode\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            with torch.no_grad():\n",
    "                for test_images, test_labels in test_loader:\n",
    "                    if train_on_gpu:\n",
    "                        test_images, test_labels = test_images.cuda(), test_labels.cuda()\n",
    "                    test_outputs = model(test_images)\n",
    "                    _, test_predicted = torch.max(test_outputs, 1)\n",
    "                    test_total += test_labels.size(0)\n",
    "                    test_correct += (test_predicted == test_labels).sum().item()\n",
    "            test_accuracy = 100 * test_correct / test_total\n",
    "            model.train()  # Set model back to train mode\n",
    "\n",
    "        metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'batch': batch_idx + 1,\n",
    "            'train_loss': batch_loss,\n",
    "            'train_accuracy': batch_accuracy,\n",
    "            'test_accuracy': test_accuracy  # This will be None if not computed\n",
    "        })\n",
    "        \n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Training Loss: {batch_loss}, Training Accuracy: {batch_accuracy}%, Test Accuracy: {'N/A' if test_accuracy is None else f'{test_accuracy}%'}\")\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    epoch_loss = train_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, End of Epoch, Training Loss: {epoch_loss}, Training Accuracy: {epoch_accuracy}%\")\n",
    "\n",
    "# Convert metrics to DataFrame and save as CSV\n",
    "df = pd.DataFrame(metrics)\n",
    "df.to_csv('training_testing_metrics.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA is not available.  Training on CPU ...\n",
      "Epoch 1/2, Batch 50/342, Training Loss: 0.515760183930397, Training Accuracy: 81.125%, Test Accuracy: 75.0%\n",
      "Epoch 1/2, Batch 100/342, Training Loss: 0.473770951628685, Training Accuracy: 82.84375%, Test Accuracy: 77.09424083769633%\n",
      "Epoch 1/2, Batch 150/342, Training Loss: 0.4549550101161003, Training Accuracy: 83.14583333333333%, Test Accuracy: 84.55497382198953%\n",
      "Epoch 1/2, Batch 200/342, Training Loss: 0.43407466158270835, Training Accuracy: 83.671875%, Test Accuracy: 85.07853403141361%\n",
      "Epoch 1/2, Batch 250/342, Training Loss: 0.41949808996915816, Training Accuracy: 84.2125%, Test Accuracy: 83.90052356020942%\n",
      "Epoch 1/2, Batch 300/342, Training Loss: 0.40777626790106297, Training Accuracy: 84.60416666666667%, Test Accuracy: 75.39267015706807%\n",
      "Epoch 1/2, End of Epoch, Training Loss: 0.39685822528479986, Training Accuracy: 84.89874461651242%\n",
      "Epoch 2/2, Batch 50/342, Training Loss: 0.2754943468421698, Training Accuracy: 88.875%, Test Accuracy: 82.32984293193718%\n",
      "Epoch 2/2, Batch 100/342, Training Loss: 0.2693746691569686, Training Accuracy: 89.4375%, Test Accuracy: 75.78534031413612%\n",
      "Epoch 2/2, Batch 150/342, Training Loss: 0.2726940515389045, Training Accuracy: 89.35416666666667%, Test Accuracy: 85.20942408376963%\n",
      "Epoch 2/2, Batch 200/342, Training Loss: 0.2757476913370192, Training Accuracy: 89.203125%, Test Accuracy: 85.07853403141361%\n",
      "Epoch 2/2, Batch 250/342, Training Loss: 0.26999555541574954, Training Accuracy: 89.325%, Test Accuracy: 83.50785340314137%\n",
      "Epoch 2/2, Batch 300/342, Training Loss: 0.26440440014004707, Training Accuracy: 89.51041666666667%, Test Accuracy: 84.16230366492147%\n",
      "Epoch 2/2, End of Epoch, Training Loss: 0.26540972384044975, Training Accuracy: 89.56290662512599%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# if not train_on_gpu:\n",
    "#     print('CUDA is not available.  Training on CPU ...')\n",
    "# else:\n",
    "#     print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "# if train_on_gpu:\n",
    "#     model.cuda()\n",
    "\n",
    "# n_epochs = 2\n",
    "# print_every = 50\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     train_loss = 0.0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "#         if train_on_gpu:\n",
    "#             images, labels = images.cuda(), labels.cuda()  # Move data to GPU\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(images)\n",
    "#         loss = criterion(output, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item() * images.size(0)\n",
    "#         _, predicted = torch.max(output, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         if (batch_idx + 1) % print_every == 0:\n",
    "#             accuracy = 100 * correct / total\n",
    "#             print(f\"Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Training Loss: {train_loss / total}, Training Accuracy: {accuracy}%\")\n",
    "    \n",
    "#     train_loss = train_loss / len(train_loader.dataset)\n",
    "#     accuracy = 100 * correct / total\n",
    "#     print(f\"Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss}, Training Accuracy: {accuracy}%\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming `model`, `train_loader`, `test_loader`, `optimizer`, `criterion` are defined\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "\n",
    "if train_on_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "n_epochs = 2\n",
    "print_every = 50\n",
    "metrics = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        if train_on_gpu:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate metrics after each batch\n",
    "        batch_accuracy = 100 * correct / total\n",
    "        batch_loss = train_loss / total\n",
    "        \n",
    "        # Optionally, evaluate test accuracy at a reduced frequency to save computation\n",
    "        test_accuracy = None\n",
    "        if (batch_idx + 1) % print_every == 0:  # Example: Calculate test accuracy every 'print_every' batches\n",
    "            model.eval()  # Set model to evaluate mode\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            with torch.no_grad():\n",
    "                for test_images, test_labels in test_loader:\n",
    "                    if train_on_gpu:\n",
    "                        test_images, test_labels = test_images.cuda(), test_labels.cuda()\n",
    "                    test_outputs = model(test_images)\n",
    "                    _, test_predicted = torch.max(test_outputs, 1)\n",
    "                    test_total += test_labels.size(0)\n",
    "                    test_correct += (test_predicted == test_labels).sum().item()\n",
    "            test_accuracy = 100 * test_correct / test_total\n",
    "            model.train()  # Set model back to train mode\n",
    "\n",
    "        metrics.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'batch': batch_idx + 1,\n",
    "            'train_loss': batch_loss,\n",
    "            'train_accuracy': batch_accuracy,\n",
    "            'test_accuracy': test_accuracy  # This will be None if not computed\n",
    "        })\n",
    "        \n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Training Loss: {batch_loss}, Training Accuracy: {batch_accuracy}%, Test Accuracy: {'N/A' if test_accuracy is None else f'{test_accuracy}%'}\")\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    epoch_loss = train_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, End of Epoch, Training Loss: {epoch_loss}, Training Accuracy: {epoch_accuracy}%\")\n",
    "\n",
    "# Convert metrics to DataFrame and save as CSV\n",
    "df = pd.DataFrame(metrics)\n",
    "df.to_csv('training_testing_metrics.csv', index=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA is not available.  Training on CPU ...\n",
      "Epoch 1/2, Batch 50/342, Training Loss: 0.3121727126836777, Training Accuracy: 88.8125%, Test Accuracy: 83.63874345549738%\n",
      "Epoch 1/2, Batch 100/342, Training Loss: 0.28443600922822954, Training Accuracy: 89.34375%, Test Accuracy: 82.85340314136126%\n",
      "Epoch 1/2, Batch 150/342, Training Loss: 0.2662510948379834, Training Accuracy: 90.04166666666667%, Test Accuracy: 82.85340314136126%\n",
      "Epoch 1/2, Batch 200/342, Training Loss: 0.2619106545299292, Training Accuracy: 89.9375%, Test Accuracy: 84.16230366492147%\n",
      "Epoch 1/2, Batch 250/342, Training Loss: 0.25401816228032115, Training Accuracy: 90.125%, Test Accuracy: 84.16230366492147%\n",
      "Epoch 1/2, Batch 300/342, Training Loss: 0.2528545100490252, Training Accuracy: 90.22916666666667%, Test Accuracy: 77.09424083769633%\n",
      "Epoch 1/2, End of Epoch, Training Loss: 0.24976556447633194, Training Accuracy: 90.35095757353615%\n",
      "Epoch 2/2, Batch 50/342, Training Loss: 0.21955051511526108, Training Accuracy: 92.0625%, Test Accuracy: 84.42408376963351%\n",
      "Epoch 2/2, Batch 100/342, Training Loss: 0.21869480296969412, Training Accuracy: 92.0625%, Test Accuracy: 84.81675392670157%\n",
      "Epoch 2/2, Batch 150/342, Training Loss: 0.2137664387623469, Training Accuracy: 91.97916666666667%, Test Accuracy: 83.63874345549738%\n",
      "Epoch 2/2, Batch 200/342, Training Loss: 0.22101965060457587, Training Accuracy: 91.5625%, Test Accuracy: 84.81675392670157%\n",
      "Epoch 2/2, Batch 250/342, Training Loss: 0.22501301382482053, Training Accuracy: 91.3375%, Test Accuracy: 84.29319371727749%\n",
      "Epoch 2/2, Batch 300/342, Training Loss: 0.22559949208050967, Training Accuracy: 91.25%, Test Accuracy: 84.29319371727749%\n",
      "Epoch 2/2, End of Epoch, Training Loss: 0.22328022485865728, Training Accuracy: 91.36809309997251%\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}