# -*- coding: utf-8 -*-
"""
Created on Sun Mar  3 14:06:15 2024

@author: arthu
"""

import torch
import torchvision.transforms as transforms
import pandas as pd
from PIL import Image
import os
from torch.utils.data import Dataset, random_split, DataLoader, WeightedRandomSampler
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report
import torch.nn.functional as F

# Check whether we have a GPU.  Use it if we do.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Navigate to the folder
data_dir = 'C:/Users/arthu/Desktop/University/EngMaths/Third_year/Intro_to_AI/lll.v1i.multiclass/all'

# Define transforms
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.7973, 0.7854, 0.7220], std=[0.2426, 0.1951, 0.2440])
])

# Create dataset class
class X_ray_dataset(Dataset):
    def __init__(self, annotations_file = os.path.join(data_dir, 'binary_classes.csv'), img_dir = os.path.join(data_dir, 'x_ray_images'), transform = None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.img_labels)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = Image.open(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        return image, label

'''    
# Test to see if the class is working
x_ray = X_ray_dataset(transform=transform)
print('len dataset:', len(x_ray)) # use the __len__ method to show the length of the dataset
example = x_ray[0] # use the __getitem__ method to get the first example
print(example[0].shape)
print('Class:', example[1])
'''

x_ray = X_ray_dataset(transform=transform)
# Define the size of the training set
train_size = int(0.85 * len(x_ray))  # 85% for training

# Split the dataset into training and testing sets
train_ds, test_ds = random_split(x_ray, [train_size, len(x_ray) - train_size])

# Split the training set into train and validation (train ~ 70% (6887), test ~ 15% (1477), val ~ 15% (1477))
train_ds, val_ds = random_split(train_ds, [6887, 1477])

# Define class labels
classes = ('Safe', 'Prohibited')

batch_size = 10

# Calculate the number of classes in train and val sets
class_0_count_train = 0
class_1_count_train = 0
class_0_count_val = 0
class_1_count_val = 0

for image, label in train_ds:  # count classes for train
    if label == 0:
        class_0_count_train += 1
    elif label == 1:
        class_1_count_train += 1

for image, label in val_ds:  # count classes for validation
    if label == 0:
        class_0_count_val += 1
    elif label == 1:
        class_1_count_val += 1
#Trying  to reformat this a bit

# Extract the class labels from the dataset
train_targets = torch.tensor([label for _, label in train_ds])
val_targets = torch.tensor([label for _, label in val_ds])

# Calculate weights for each class
class_sample_count = torch.tensor([class_0_count_train, class_1_count_train])
weight = 1. / class_sample_count.float()
samples_weight = torch.tensor([weight[t] for t in train_targets])

# WeightedRandomSampler for training set
train_sampler = WeightedRandomSampler(weights=samples_weight, num_samples=len(samples_weight), replacement=True)

# For validation, you can repeat the process if you want stratified sampling there as well
val_class_sample_count = torch.tensor([class_0_count_val, class_1_count_val])
val_weight = 1. / val_class_sample_count.float()
val_samples_weight = torch.tensor([val_weight[t] for t in val_targets])

val_sampler = WeightedRandomSampler(weights=val_samples_weight, num_samples=len(val_samples_weight), replacement=True)

# Data loaders with stratified sampling
train_loader = DataLoader(train_ds,
                          batch_size=10,
                          sampler=train_sampler,
                          shuffle = False)  # don't shuffle with stratified sampling
test_loader = DataLoader(test_ds,
                         batch_size = batch_size*5,
                         shuffle = False)
val_loader = DataLoader(val_ds,
                        batch_size=10*5,
                        sampler=val_sampler,
                        shuffle = False)


# Create CNN
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        # Convolutional layers
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.pool1 = nn.MaxPool2d(2, 2)  # Adding pooling layer
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.pool2 = nn.MaxPool2d(2, 2)  # Adding pooling layer
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(64)
        self.pool3 = nn.MaxPool2d(2, 2)  # Adding pooling layer
        
        # After 3 rounds of halving the dimensions, the size calculation needs adjustment
        # For an input of 224x224, after three poolings, the size is 224 / 2 / 2 / 2 = 28
        self.fc1_size = 64 * 28 * 28  # Adjusted based on the pooling layers
        self.fc1 = nn.Linear(self.fc1_size, 500)
        self.fc2 = nn.Linear(500, 8)
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))  # Apply pooling
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))  # Apply pooling
        x = self.pool3(F.relu(self.bn3(self.conv3(x))))  # Apply pooling
        x = x.view(-1, self.fc1_size)  # Flatten the output for the fully connected layer
        x = self.dropout(x)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Instantiate Network
net = CNN()
net.to(device)

'''
# Checking the shape of an image before and after the network
for i, data in enumerate(train_loader):
  inputs, labels = data[0].to(device), data[1].to(device)
  print('Input shape:', inputs.shape)
  print('Output shape:', net(inputs).shape)
  break
'''

# Specifying hyperparameters

class CustomCrossEntropyLoss(nn.Module):
    def __init__(self):
        super(CustomCrossEntropyLoss, self).__init__()

    def forward(self, input, target):
        log_softmax_values = torch.log_softmax(input, dim=1)

        # Calculate class weights
        class_weights = torch.tensor([1.0 / count for count in torch.bincount(target)], device=input.device)

        # Weight the losses by class weights
        weighted_losses = -target * log_softmax_values * class_weights[target]

        return torch.mean(torch.sum(weighted_losses, dim=1))


#loss_func = CustomCrossEntropyLoss()
loss_func = nn.CrossEntropyLoss()
optimiser = optim.Adam(net.parameters(), lr = 0.001, weight_decay = 1e-6)

# Function for training one epoch

train_accuracies = []
train_losses = []
metrics = []
train_metrics = []
print_every = 50

def train_epoch():
  net.train(True) # set to training mode

  # Metrics that will build up
  running_loss = 0
  running_accuracy = 0

  # Iterate over train data
  for batch_index, data in enumerate(train_loader):
    inputs, labels = data[0].to(device), data[1].to(device) # get the images and labels

    optimiser.zero_grad() # set all non-zero values for gradients to 0 (reset gradients)

    outputs = net(inputs).squeeze((-1, -2)) # shape: [batch size, 2]
    correct_prediction = torch.sum(labels == torch.argmax(outputs, dim = 1)).item() # check how many images the model predicted correctly
    running_accuracy += correct_prediction/batch_size # update the accuracy

    # Train model
    loss = loss_func(outputs, labels) # compare model predictions with labels
    running_loss += loss.item() # update the loss
    loss.backward() # calculate gradients
    optimiser.step()
    
    train_acc = running_accuracy/(batch_index+1) * 100
    train_loss = running_loss/(batch_index+1)
    
    if batch_index % 50 == 49:      # print for every 50 batchs
      avg_loss = running_loss/(batch_index + 1)  # get the average loss across batches
      avg_acc = (running_accuracy/(batch_index +1)) * 100 # get the average accuracy across batches
      print('Batch {0}, Loss: {1:.3f}, Accuracy: {2:.1f}%'.format(batch_index+1, avg_loss, avg_acc))
      train_accuracies.append(avg_acc)
      train_losses.append(avg_loss)
      
    # evaluate val acc
    val_acc = None
    val_loss = None
    if (batch_index + 1) % print_every == 0:
        net.eval()
        val_correct = 0
        val_total = 0
        val_running_loss = 0
        with torch.no_grad():
            for val_images, val_labels in val_loader:
                val_outputs = net(val_images).squeeze((-1, -2))
                _, val_predicted = torch.max(val_outputs, 1)
                val_total += val_labels.size(0)
                val_correct += (val_predicted == val_labels).sum().item()
                loss = loss_func(val_outputs, val_labels) # compare model predictions with labels
                val_running_loss += loss.item() # update the loss
        
        val_acc = 100*val_correct/val_total
        val_loss = val_running_loss/50
        
        net.train()
    
    metrics.append({
        'Batch': batch_index + 1,
        'val_accuracy': val_acc,
        'val_loss': val_loss,
        'train_accuracy': train_acc,
        'train_loss': train_loss})
        
    
  print()


# Function for testing
test_accuracies = []

def test_epoch(epoch):
  with torch.no_grad():
    correct_prediction = 0
    total = 0
    for images, labels in test_loader:
      images = images.to(device)
      labels = labels.to(device)

      outputs = net(images).squeeze((-1,-2))

      predicted = torch.argmax(outputs, -1)
      correct_prediction += (predicted == labels).sum().item()
      total += labels.size(0)

    acc =(correct_prediction/total) * 100
    
    # add the accuracy to a list 
    test_accuracies.append(acc)

    print('Test accuracy after {0:.0f} epoch(s): {1:.1f}%'.format(epoch+1, acc))
    print()

# Function for validating one epoch
val_accuracies = []
val_losses = []

def validate_epoch():
  net.train(False) # set to evaluation mode
  running_loss = 0
  running_acc = 0

  # Iterate over validation data
  for batch_index, data in enumerate(val_loader):
    inputs, labels = data[0].to(device), data[1].to(device)

    with torch.no_grad(): # not worried about gradients here as not training
      outputs = net(inputs).squeeze((-1, -2)) # shape [batch size, 2]
      correct_prediction = torch.sum(labels == torch.argmax(outputs, dim = 1)).item() # check how many images the model predicted correctly
      running_acc += correct_prediction/(batch_size*5) # update the accuracy
      loss = loss_func(outputs, labels) # compare model predictions with labels
      running_loss += loss.item() # update the loss

  avg_loss = running_loss/len(val_loader)
  avg_acc = (running_acc/len(val_loader)) * 100
  
  # add the accuracy and loss to a list
  val_accuracies.append(avg_acc)
  val_losses.append(avg_loss)

  print('Val Loss: {0:.3f}, Val Accuracy: {1:.1f}%'.format(avg_loss, avg_acc))

  print('-----------------------------------------------------------')
  print()

# Training loop

num_epochs = 6

for i in range(num_epochs):
  print('Epoch:', i+1, '\n')

  train_epoch()
  test_epoch(i)
  validate_epoch()

print('Finished Training')

# Create the confusion matrix
net.eval()
y_true = []
y_pred = []

with torch.no_grad():
  for images, labels in test_loader:
    images = images.to(device)
    labels = labels.to(device)
    outputs = net(images)
    _, predicted = torch.max(outputs, 1)
    y_true += labels.tolist()
    y_pred += predicted.tolist()

#y_pred_flattened = [item for sublist in y_pred for subsublist in sublist for item in subsublist]

cm = confusion_matrix(y_true, y_pred)
cm_df = pd.DataFrame(cm, index=classes, columns=classes)
print(cm)
plt.figure(figsize=(10, 8))
sns.heatmap(cm_df, annot=True, cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Plotting the loss graph
df1 = pd.DataFrame(metrics)
df1.to_csv('metrics_CrossEntropy_Loss.csv', index = False)

# batch_length = df['batch'].apply(len)
# test_acc = df['test_accuracy']

# plt.figure(2)
# plt.plot(range(batch_length), test_acc, label='Test Accuracy')
# plt.xlabel('Batch')
# plt.ylabel('Accuracy')
# plt.title('Testing Accuracy')
# plt.legend()
# plt.show()

# Plotting accuracy and loss graphs
# epochs = range(1, len(val_accuracies) + 1)
# plt.figure(2)
# plt.plot(epochs, val_accuracies, label = 'Validation Accuracy')
# plt.plot(epochs, test_accuracies, label = 'Test Accuracy')
# plt.title('Validation and Test Accuracies vs Number of Epochs')
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')
# plt.legend()
# plt.grid(True)
# plt.show()

# plt.figure(3)
# plt.plot(epochs, val_losses, label = 'Validation Loss')
# plt.title('Validation loss vs Number of Epochs')
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.grid(True)
# plt.show()

# Generate a classification report using the predicted and actual labels
class_report = classification_report(y_true, y_pred, target_names=['Safe', 'Prohibited'])

# Print the classification report
print("Classification Report:\n", class_report)
# Print the classification report
print("Classification Report:\n", class_report)

